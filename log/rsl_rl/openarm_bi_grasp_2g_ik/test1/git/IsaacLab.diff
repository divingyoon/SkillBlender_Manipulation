--- git status ---
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   scripts/reinforcement_learning/rl_games/play.py
	modified:   scripts/reinforcement_learning/rl_games/train.py
	modified:   scripts/reinforcement_learning/rsl_rl/play.py
	modified:   scripts/reinforcement_learning/rsl_rl/train.py
	modified:   source/isaaclab/isaaclab/managers/reward_manager.py
	modified:   source/isaaclab_rl/isaaclab_rl/rsl_rl/vecenv_wrapper.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	source/isaaclab_rl/isaaclab_rl/rsl_rl/vecenv_wrapper.py.bak_20260121_141307

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/scripts/reinforcement_learning/rl_games/play.py b/scripts/reinforcement_learning/rl_games/play.py
index 135980e92c..6ea1485e44 100644
--- a/scripts/reinforcement_learning/rl_games/play.py
+++ b/scripts/reinforcement_learning/rl_games/play.py
@@ -82,6 +82,12 @@ import isaaclab_tasks  # noqa: F401
 from isaaclab_tasks.utils import get_checkpoint_path
 from isaaclab_tasks.utils.hydra import hydra_task_config
 
+# Load optional extensions (e.g., openarm) so their Gym envs register with Hydra.
+try:
+    import openarm.tasks  # noqa: F401
+except ImportError:
+    pass
+
 # PLACEHOLDER: Extension template (do not remove this comment)
 
 
diff --git a/scripts/reinforcement_learning/rl_games/train.py b/scripts/reinforcement_learning/rl_games/train.py
index d44d03e14e..44ab3d1dd7 100644
--- a/scripts/reinforcement_learning/rl_games/train.py
+++ b/scripts/reinforcement_learning/rl_games/train.py
@@ -90,6 +90,12 @@ from isaaclab_rl.rl_games import MultiObserver, PbtAlgoObserver, RlGamesGpuEnv,
 import isaaclab_tasks  # noqa: F401
 from isaaclab_tasks.utils.hydra import hydra_task_config
 
+# Load optional extensions (e.g., openarm) so their Gym envs register with Hydra.
+try:
+    import openarm.tasks  # noqa: F401
+except ImportError:
+    pass
+
 # import logger
 logger = logging.getLogger(__name__)
 
diff --git a/scripts/reinforcement_learning/rsl_rl/play.py b/scripts/reinforcement_learning/rsl_rl/play.py
index fe988508ef..a4c5334cc2 100644
--- a/scripts/reinforcement_learning/rsl_rl/play.py
+++ b/scripts/reinforcement_learning/rsl_rl/play.py
@@ -74,6 +74,7 @@ from isaaclab.utils.pretrained_checkpoint import get_published_pretrained_checkp
 from isaaclab_rl.rsl_rl import RslRlBaseRunnerCfg, RslRlVecEnvWrapper, export_policy_as_jit, export_policy_as_onnx
 
 import isaaclab_tasks  # noqa: F401
+import openarm.tasks  # noqa: F401
 from isaaclab_tasks.utils import get_checkpoint_path
 from isaaclab_tasks.utils.hydra import hydra_task_config
 
diff --git a/scripts/reinforcement_learning/rsl_rl/train.py b/scripts/reinforcement_learning/rsl_rl/train.py
index 888b8d86a6..2f6caa8952 100644
--- a/scripts/reinforcement_learning/rsl_rl/train.py
+++ b/scripts/reinforcement_learning/rsl_rl/train.py
@@ -97,6 +97,7 @@ from isaaclab.utils.io import dump_yaml
 from isaaclab_rl.rsl_rl import RslRlBaseRunnerCfg, RslRlVecEnvWrapper
 
 import isaaclab_tasks  # noqa: F401
+import openarm.tasks  # noqa: F40
 from isaaclab_tasks.utils import get_checkpoint_path
 from isaaclab_tasks.utils.hydra import hydra_task_config
 
diff --git a/source/isaaclab/isaaclab/managers/reward_manager.py b/source/isaaclab/isaaclab/managers/reward_manager.py
index 63077eacc4..9b51da850e 100644
--- a/source/isaaclab/isaaclab/managers/reward_manager.py
+++ b/source/isaaclab/isaaclab/managers/reward_manager.py
@@ -143,7 +143,10 @@ class RewardManager(ManagerBase):
         for term_idx, (name, term_cfg) in enumerate(zip(self._term_names, self._term_cfgs)):
             # skip if weight is zero (kind of a micro-optimization)
             if term_cfg.weight == 0.0:
-                self._step_reward[:, term_idx] = 0.0
+                # log-only term: record raw values without affecting total reward
+                raw_value = term_cfg.func(self._env, **term_cfg.params)
+                self._episode_sums[name] += raw_value * dt
+                self._step_reward[:, term_idx] = raw_value
                 continue
             # compute term's value
             value = term_cfg.func(self._env, **term_cfg.params) * term_cfg.weight * dt
diff --git a/source/isaaclab_rl/isaaclab_rl/rsl_rl/vecenv_wrapper.py b/source/isaaclab_rl/isaaclab_rl/rsl_rl/vecenv_wrapper.py
index 73ceae0469..137bb9ccaa 100644
--- a/source/isaaclab_rl/isaaclab_rl/rsl_rl/vecenv_wrapper.py
+++ b/source/isaaclab_rl/isaaclab_rl/rsl_rl/vecenv_wrapper.py
@@ -24,7 +24,15 @@ class RslRlVecEnvWrapper(VecEnv):
         https://github.com/leggedrobotics/rsl_rl/blob/master/rsl_rl/env/vec_env.py
     """
 
-    def __init__(self, env: ManagerBasedRLEnv | DirectRLEnv, clip_actions: float | None = None):
+    def __init__(
+        self,
+        env: ManagerBasedRLEnv | DirectRLEnv,
+        clip_actions: float | None = None,
+        swap_lr: bool = False,
+        swap_prob: float = 0.5,
+        swap_obs_term_pairs: list[tuple[str, str]] | None = None,
+        swap_action_term_pairs: list[tuple[str, str]] | None = None,
+    ):
         """Initializes the wrapper.
 
         Note:
@@ -48,6 +56,8 @@ class RslRlVecEnvWrapper(VecEnv):
         # initialize the wrapper
         self.env = env
         self.clip_actions = clip_actions
+        self._swap_lr = swap_lr
+        self._swap_prob = float(swap_prob)
 
         # store information required by wrapper
         self.num_envs = self.unwrapped.num_envs
@@ -63,6 +73,28 @@ class RslRlVecEnvWrapper(VecEnv):
         # modify the action space to the clip range
         self._modify_action_space()
 
+        # prepare swap helpers (if enabled)
+        self._swap_mask = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
+        self._obs_term_slices: dict[str, dict[str, slice]] = {}
+        self._obs_group_concat: dict[str, bool] = {}
+        self._action_term_slices: dict[str, slice] = {}
+        self._reward_term_pairs: list[tuple[str, str]] = []
+        self._reward_term_indices: list[tuple[int, int]] = []
+
+        self._swap_obs_term_pairs = swap_obs_term_pairs or [
+            ("target_object_position", "target_object2_position"),
+            ("object_position", "object2_position"),
+            ("object_obs", "object2_obs"),
+        ]
+        self._swap_action_term_pairs = swap_action_term_pairs or [
+            ("left_arm_action", "right_arm_action"),
+            ("left_hand_action", "right_hand_action"),
+        ]
+
+        if self._swap_lr:
+            self._build_swap_helpers()
+            self._sample_swap_mask()
+
         # reset at the start since the RSL-RL runner does not call reset
         self.env.reset()
 
@@ -139,6 +171,9 @@ class RslRlVecEnvWrapper(VecEnv):
     def reset(self) -> tuple[TensorDict, dict]:  # noqa: D102
         # reset the environment
         obs_dict, extras = self.env.reset()
+        if self._swap_lr:
+            self._sample_swap_mask()
+            self._swap_obs_inplace(obs_dict, self._swap_mask)
         return TensorDict(obs_dict, batch_size=[self.num_envs]), extras
 
     def get_observations(self) -> TensorDict:
@@ -153,10 +188,21 @@ class RslRlVecEnvWrapper(VecEnv):
         # clip actions
         if self.clip_actions is not None:
             actions = torch.clamp(actions, -self.clip_actions, self.clip_actions)
+        # swap actions for mirrored environments
+        if self._swap_lr:
+            actions = self._swap_actions_inplace(actions, self._swap_mask)
         # record step information
         obs_dict, rew, terminated, truncated, extras = self.env.step(actions)
         # compute dones for compatibility with RSL-RL
         dones = (terminated | truncated).to(dtype=torch.long)
+        if self._swap_lr:
+            # swap reward terms/logs using current mask (pre-reset)
+            self._swap_reward_terms_inplace(self._swap_mask)
+            # resample swap mask for envs that just ended
+            if torch.any(dones.bool()):
+                self._sample_swap_mask(dones.bool())
+            # swap observations for current mask (post-reset for done envs)
+            self._swap_obs_inplace(obs_dict, self._swap_mask)
         # move time out information to the extras dict
         # this is only needed for infinite horizon tasks
         if not self.unwrapped.cfg.is_finite_horizon:
@@ -171,6 +217,121 @@ class RslRlVecEnvWrapper(VecEnv):
     Helper functions
     """
 
+    def _build_swap_helpers(self):
+        """Pre-compute term slices for swapping."""
+        if not hasattr(self.unwrapped, "observation_manager"):
+            return
+
+        obs_mgr = self.unwrapped.observation_manager
+        for group_name, term_names in obs_mgr.active_terms.items():
+            self._obs_group_concat[group_name] = obs_mgr.group_obs_concatenate[group_name]
+            if not obs_mgr.group_obs_concatenate[group_name]:
+                continue
+            term_dims = obs_mgr.group_obs_term_dim[group_name]
+            term_slices: dict[str, slice] = {}
+            idx = 0
+            for name, dims in zip(term_names, term_dims):
+                length = int(torch.prod(torch.tensor(dims)).item())
+                term_slices[name] = slice(idx, idx + length)
+                idx += length
+            self._obs_term_slices[group_name] = term_slices
+
+        if hasattr(self.unwrapped, "action_manager"):
+            names = self.unwrapped.action_manager.active_terms
+            dims = self.unwrapped.action_manager.action_term_dim
+            idx = 0
+            for name, dim in zip(names, dims):
+                self._action_term_slices[name] = slice(idx, idx + int(dim))
+                idx += int(dim)
+
+        if hasattr(self.unwrapped, "reward_manager"):
+            reward_terms = set(self.unwrapped.reward_manager.active_terms)
+            for name in reward_terms:
+                if "left_" in name:
+                    counterpart = name.replace("left_", "right_", 1)
+                    if counterpart in reward_terms:
+                        self._reward_term_pairs.append((name, counterpart))
+            if self._reward_term_pairs:
+                name_to_idx = {n: i for i, n in enumerate(self.unwrapped.reward_manager.active_terms)}
+                for left, right in self._reward_term_pairs:
+                    self._reward_term_indices.append((name_to_idx[left], name_to_idx[right]))
+
+    def _sample_swap_mask(self, env_ids: torch.Tensor | None = None):
+        """Sample swap mask per environment (per-episode)."""
+        if env_ids is None:
+            env_ids = torch.ones(self.num_envs, dtype=torch.bool, device=self.device)
+        count = int(env_ids.sum().item())
+        if count == 0:
+            return
+        rand = torch.rand((count,), device=self.device)
+        self._swap_mask[env_ids] = rand < self._swap_prob
+
+    def _swap_actions_inplace(self, actions: torch.Tensor, swap_mask: torch.Tensor) -> torch.Tensor:
+        if not torch.any(swap_mask):
+            return actions
+        if not self._action_term_slices:
+            return actions
+        for left, right in self._swap_action_term_pairs:
+            if left not in self._action_term_slices or right not in self._action_term_slices:
+                continue
+            left_slice = self._action_term_slices[left]
+            right_slice = self._action_term_slices[right]
+            tmp = actions[swap_mask, left_slice].clone()
+            actions[swap_mask, left_slice] = actions[swap_mask, right_slice]
+            actions[swap_mask, right_slice] = tmp
+        return actions
+
+    def _swap_obs_inplace(self, obs_dict: dict, swap_mask: torch.Tensor):
+        if not torch.any(swap_mask):
+            return
+        for group_name, group_obs in obs_dict.items():
+            if group_name not in self._obs_group_concat:
+                continue
+            if self._obs_group_concat[group_name]:
+                if group_name not in self._obs_term_slices:
+                    continue
+                term_slices = self._obs_term_slices[group_name]
+                for left, right in self._swap_obs_term_pairs:
+                    if left not in term_slices or right not in term_slices:
+                        continue
+                    left_slice = term_slices[left]
+                    right_slice = term_slices[right]
+                    tmp = group_obs[swap_mask, left_slice].clone()
+                    group_obs[swap_mask, left_slice] = group_obs[swap_mask, right_slice]
+                    group_obs[swap_mask, right_slice] = tmp
+            else:
+                if not isinstance(group_obs, dict):
+                    continue
+                for left, right in self._swap_obs_term_pairs:
+                    if left not in group_obs or right not in group_obs:
+                        continue
+                    tmp = group_obs[left][swap_mask].clone()
+                    group_obs[left][swap_mask] = group_obs[right][swap_mask]
+                    group_obs[right][swap_mask] = tmp
+
+    def _swap_reward_terms_inplace(self, swap_mask: torch.Tensor):
+        if not torch.any(swap_mask):
+            return
+        if not hasattr(self.unwrapped, "reward_manager"):
+            return
+        reward_manager = self.unwrapped.reward_manager
+        if self._reward_term_indices and hasattr(reward_manager, "_step_reward"):
+            for left_idx, right_idx in self._reward_term_indices:
+                tmp = reward_manager._step_reward[swap_mask, left_idx].clone()
+                reward_manager._step_reward[swap_mask, left_idx] = reward_manager._step_reward[
+                    swap_mask, right_idx
+                ]
+                reward_manager._step_reward[swap_mask, right_idx] = tmp
+        if self._reward_term_pairs and hasattr(reward_manager, "_episode_sums"):
+            for left_name, right_name in self._reward_term_pairs:
+                if left_name not in reward_manager._episode_sums or right_name not in reward_manager._episode_sums:
+                    continue
+                tmp = reward_manager._episode_sums[left_name][swap_mask].clone()
+                reward_manager._episode_sums[left_name][swap_mask] = reward_manager._episode_sums[right_name][
+                    swap_mask
+                ]
+                reward_manager._episode_sums[right_name][swap_mask] = tmp
+
     def _modify_action_space(self):
         """Modifies the action space to the clip range."""
         if self.clip_actions is None: